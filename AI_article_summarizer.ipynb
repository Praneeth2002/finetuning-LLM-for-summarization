{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b773fd6",
   "metadata": {},
   "source": [
    "# AI 574 Final Project\n",
    "## Article Summarization fine tuned for articles about deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b52af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d797aa",
   "metadata": {},
   "source": [
    "# Section 1:  Define and use functions to retrieve and parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3704d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec653e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a function that walks through the full directory and finds all of the xml files.\n",
    "#  This function isn't too hard.  Provided with a root directory, it creates a blank list and then \n",
    "#  walks down the subdirectories.  Every time it finds an xml file, it adds it to the list.\n",
    "def list_xml_files(directory):\n",
    "    xml_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                xml_files.append(os.path.join(root, file))\n",
    "    return xml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a12b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we start defining functions to parse the XML file itsself.  For every header at level one that we find \n",
    "#  within the XML file, we need to pull it's title.  The ABSTRACT seems to pull out correctly, but for any further\n",
    "#  major sections in the paper, the attribute of \"title\" ends up holding the title of the section.  As such, we use\n",
    "#  the element tag to hold the section title if it doesn't have a specific title.  If it does, then the title is held.\n",
    "#  This function creates a list of the data for the entire paper.  Specifically, if the depth of the XML tree is at 1, it\n",
    "#  iterates through the children of this tree and pulls out all of the text from the file.  This is then appended to a list\n",
    "#  to store all of the data.\n",
    "\n",
    "def parse_element(element, depth = 0):\n",
    "    data = []\n",
    "    \n",
    "    if depth == 1 :\n",
    "        if element.tag == 'SECTION':\n",
    "            header = element.attrib.get('title')\n",
    "        else:\n",
    "            header = element.tag\n",
    "        content = []\n",
    "        \n",
    "        for child in element:\n",
    "            content.append(child.text.strip() if child.text else '')\n",
    "        \n",
    "        if content:\n",
    "            data.append((header, '\\n'.join(content)))\n",
    "    \n",
    "    for child in element:\n",
    "        data.extend(parse_element(child, depth + 1))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d8f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use the prse_element function, we need to retrieve the file from the tree\n",
    "# and then apply the parsing function to the root retrieved from the path to that paper\n",
    "\n",
    "def parse_xml_file(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    return parse_element(root, depth = 0)\n",
    "\n",
    "#  For each of these we also need to read a text file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as file:\n",
    "        return file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2178b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we need to join all of the section headers and text into a single text item.\n",
    "#  This item will fill our dictionary text item for the articles that we are training from.\n",
    "\n",
    "def format_to_text(data):\n",
    "    formatted_text = []\n",
    "    \n",
    "    for header, content in data:\n",
    "        formatted_text.append(header)\n",
    "        formatted_text.append(content)\n",
    "    return '\\n\\n'.join(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3bd9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(xml_file_path):\n",
    "    base_path = os.path.dirname(xml_file_path)\n",
    "    xml_file_name = os.path.basename(xml_file_path).replace('.xml', '')\n",
    "    summary_file_path = os.path.join(base_path, '..', 'summary', f'{xml_file_name}.gold.txt')\n",
    "    \n",
    "    xml_data = parse_xml_file(xml_file)\n",
    "    document_text = format_to_text(xml_data)\n",
    "    \n",
    "    summary_text = read_text_file(summary_file_path)\n",
    "    \n",
    "    lines = summary_text.split('\\n', 1)\n",
    "    title = lines[0].strip() if lines else ''\n",
    "    summary = lines[1].strip() if len(lines) > 1 else ''\n",
    "    \n",
    "    results = {\n",
    "        'text': document_text,\n",
    "        'summary': summary,\n",
    "        'title': title\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4df620",
   "metadata": {},
   "source": [
    "## Reading the documents:\n",
    "There are 1009 documents in our sample dataset, so we need to generate a list of the files that store those documents.  \n",
    "This code will generate a list of the links to the xml files.  These file names are then used to track down the summaries as well with the functions presented above.  In the end, we end up with a list of dictionary items containing the text, summary, and title for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c08ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"top1000_complete\"\n",
    "xml_files = list_xml_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa428c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009\n"
     ]
    }
   ],
   "source": [
    "print(len(xml_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a39209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'top1000_complete\\\\A00-1043\\\\Documents_xml\\\\A00-1043.xml'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_files[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec633f",
   "metadata": {},
   "source": [
    "#### An example of a single paper in order to make sure that the above code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4f0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_files(xml_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175fa97",
   "metadata": {},
   "source": [
    "###  Reading all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b7810be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for xml_file in xml_files:\n",
    "    file = read_files(xml_file)\n",
    "    data.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bafd2",
   "metadata": {},
   "source": [
    "Our data is now in a list of dictionaries which matches the billsum dataset within the tutorial provided by huggingface at https://huggingface.co/docs/transformers/v4.17.0/en/tasks/summarization\n",
    "\n",
    "Based on this, we should be able to fine tune our model.  One quick step that we should take will be to convert our dataset into the right format and then apply a splitting to the data in order to test the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f788fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3da464",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\n",
    "    'text': [item['text'] for item in data],\n",
    "    'summary': [item['summary'] for item in data],\n",
    "    'title': [item['title'] for item in data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b7e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_temp, test_data = dataset.train_test_split(test_size = 0.15, seed = 42).values()\n",
    "train_data, valid_data = train_data_temp.train_test_split(test_size = 0.176, seed = 42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130e50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'valid': valid_data,\n",
    "    'test': test_data\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "571243c5-52ba-4149-bdf3-6a3240be4536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n",
      "151\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dict['train']))\n",
    "print(len(data_dict['valid']))\n",
    "print(len(data_dict['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b989fc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"ABSTRACT\\n\\nThis paper presents and compares WordNetbased and distributional similarity approaches.\\nThe strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.\\nEach of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.\\nFinally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.\\n\\n1 Introduction\\n\\nMeasuring semantic similarity and relatedness between terms is an important problem in lexical semantics.\\nIt has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval.\\nThe techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007).\\nIn this paper, we explore both families.\\nFor the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus.\\nPrevious work suggests that distributional similarities suffer from certain limitations, which make them less useful than knowledge resources for semantic similarity.\\nFor example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora.\\nComparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets.\\nAn application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information Retrieval or CL sponsored search.\\nA discussion on the differences between learning similarity and relatedness scores is provided.\\nThe paper is structured as follows.\\nWe first present the WordNet-based method, followed by the distributional methods.\\nSection 4 is devoted to the evaluation and results on the monolingual and crosslingual tasks.\\nSection 5 presents some analysis, including learning curves for distributional methods, the use of distributional similarity to improve WordNet similarity, the contrast between similarity and relatedness, and the combination of methods.\\nSection 6 presents related work, and finally, Section 7 draws the conclusions and mentions future work.\\n\\n2 WordNet-based method\\n\\nWordNet (Fellbaum, 1998) is a lexical database of English, which groups nouns, verbs, adjectives and adverbs into sets of synonyms (synsets), each expressing a distinct concept.\\nSynsets are interlinked with conceptual-semantic and lexical relations, including hypernymy, meronymy, causality, etc.\\nGiven a pair of words and a graph-based representation of WordNet, our method has basically two steps: We first compute the personalized PageRank over WordNet separately for each of the words, producing a probability distribution over WordNet synsets.\\nWe then compare how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors.\\nWe represent WordNet as a graph G = (V, E) as follows: graph nodes represent WordNet concepts (synsets) and dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges.\\nFor each word in the pair we first compute a personalized PageRank vector of graph G (Haveliwala, 2002).\\nBasically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation.\\nIn our case, we concentrate all probability mass in the target word.\\nRegarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations.\\nThese are default values, and we did not optimize them.\\nOur similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets.\\nMore details of our algorithm can be found in (Agirre and Soroa, 2009).\\nThe algorithm and needed resouces are publicly available1.\\nThe WordNet versions that we use in this work are the Multilingual Central Repository or MCR (Atserias et al., 2004) (which includes English WordNet version 1.6 and wordnets for several other languages like Spanish, Italian, Catalan and Basque), and WordNet version 3.02.\\nWe used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0.\\nGiven the recent availability of the disambiguated gloss relations for WordNet 3.03, we also used a version which incorporates these relations.\\nWe will refer to the three versions as MCR16, WN30 and WN30g, respectively.\\nOur choice was mainly motivated by the fact that MCR contains tightly aligned wordnets of several languages (see below).\\nMCR follows the EuroWordNet design (Vossen, 1998), which specifies an InterLingual Index (ILI) that links the concepts across wordnets of different languages.\\nThe wordnets for other languages in MCR use the English WordNet synset numbers as ILIs.\\nThis design allows a decoupling of the relations between concepts (which can be taken to be language independent) and the links from each content word to its corresponding concepts (which is language dependent).\\nAs our WordNet-based method uses the graph of the concepts and relations, we can easily compute the similarity between words from different languages.\\nFor example, consider a English-Spanish pair like car – coche.\\nGiven that the Spanish WordNet is included in MCR we can use MCR as the common knowledge-base for the relations.\\nWe can then compute the personalized PageRank for each of car and coche on the same underlying graph, and then compare the similarity between both probability distributions.\\nAs an alternative, we also tried to use publicly available mappings for wordnets (Daude et al., 2000)4 in order to create a 3.0 version of the Spanish WordNet.\\nThe mapping was used to link Spanish variants to 3.0 synsets.\\nWe used the English WordNet 3.0, including glosses, to construct the graph.\\nThe two Spanish WordNet versions are referred to as MCR16 and WN30g.\\n\\n3 Context-based methods\\n\\nIn this section, we describe the distributional methods used for calculating similarities between words, and profiting from the use of a large Web-based corpus.\\nThis work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words.\\nTurney (2001) uses the number of hits returned by a Web search engine to calculate the Pointwise Mutual Information (PMI) between terms, as an indicator of synonymy.\\nBollegala et al. (2007) calculate a number of popular relatedness metrics based on page counts, like PMI, the Jaccard coefficient, the Simpson coefficient and the Dice coefficient, which are combined with lexico-syntactic patterns as model features.\\nThe model parameters are trained using Support Vector Machines (SVM) in order to later rank pairs of words.\\nA different approach is the one taken by Sahami and Heilman (2006), who collect snippets from the results of a search engine and represent each snippet as a vector, weighted with the tf·idf score.\\nThe semantic similarity between two queries is calculated as the inner product between the centroids of the respective sets of vectors.\\nTo calculate the similarity of two words w1 and w2, Ruiz-Casado et al. (2005) collect snippets containing w1 from a Web search engine, extract a context around it, replace it with w2 and check for the existence of that modified context in the Web.\\nUsing a search engine to calculate similarities between words has the drawback that the data used will always be truncated.\\nSo, for example, the numbers of hits returned by search engines nowadays are always approximate and rounded up.\\nThe systems that rely on collecting snippets are also limited by the maximum number of documents returned per query, typically around a thousand.\\nWe hypothesize that by crawling a large corpus from the Web and doing standard corpus analysis to collect precise statistics for the terms we should improve over other unsupervised systems that are based on search engine results, and should yield results that are competitive even when compared to knowledge-based approaches.\\nIn order to calculate the semantic similarity between the words in a set, we have used a vector space model, with the following three variations: In the bag-of-words approach, for each word w in the dataset we collect every term t that appears in a window centered in w, and add them to the vector together with its frequency.\\nIn the context window approach, for each word w in the dataset we collect every window W centered in w (removing the central word), and add it to the vector together with its frequency (the total number of times we saw window W around w in the whole corpus).\\nIn this case, all punctuation symbols are replaced with a special token, to unify patterns like, the <term> said to and ’ the <term> said to.\\nThroughout the paper, when we mention a context window of size N it means N words at each side of the phrase of interest.\\nIn the syntactic dependency approach, we parse the entire corpus using an implementation of an Inductive Dependency parser as described in Nivre (2006).\\nFor each word w we collect a template of the syntactic context.\\nWe consider sequences of governing words (e.g. the parent, grand-parent, etc.) as well as collections of descendants (e.g., immediate children, grandchildren, etc.).\\nThis information is then encoded as a contextual template.\\nFor example, the context template cooks <term> delicious could be contexts for nouns such as food, meals, pasta, etc.\\nThis captures both syntactic preferences as well as selectional preferences.\\nContrary to Pado and Lapata (2007), we do not use the labels of the syntactic dependencies.\\nOnce the vectors have been obtained, the frequency for each dimension in every vector is weighted using the other vectors as contrast set, with the k2 test, and finally the cosine similarity between vectors is used to calculate the similarity between each pair of terms.\\nExcept for the syntactic dependency approach, where closed-class words are needed by the parser, in the other cases we have removed stopwords (pronouns, prepositions, determiners and modal and auxiliary verbs).\\nWe have used a corpus of four billion documents, crawled from the Web in August 2008.\\nAn HTML parser is used to extract text, the language of each document is identified, and non-English documents are discarded.\\nThe final corpus remaining at the end of this process contains roughly 1.6 Terawords.\\nAll calculations are done in parallel sharding by dimension, and it is possible to calculate all pairwise similarities of the words in the test sets very quickly on this corpus using the MapReduce infrastructure.\\nA complete run takes around 15 minutes on 2,000 cores.\\nIn order to calculate similarities in a cross-lingual setting, where some of the words are in a language l other than English, the following algorithm is used: models and distributional models.\\nCW=Context Windows, BoW=bag of words, Syn=syntactic vectors.\\nFor Syn, the window size is actually the tree-depth for the governors and descendants.\\nFor examples, G1 indicates that the contexts include the parents and D2 indicates that both the children and grandchildren make up the contexts.\\nThe final grouping includes both contextual windows (at width 4) and syntactic contexts in the template vectors.\\nMax scores are bolded.\\n\\n4 Experimental results\\n\\nWe have used two standard datasets.\\nThe first one, RG, consists of 65 pairs of words collected by Rubenstein and Goodenough (1965), who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might appear between the terms.\\nThe second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.\\nIn this case, both similarity and rell never forget the' on his face when grin,2,smile,10 he had a giant' on his face and grin,3,smile,2 room with a huge' on her face and grin,2,smile,6 the state of every' will be updated every automobile,2,car,3 repair or replace the' if it is stolen automobile,2,car,2 located on the north' of the Bay of shore,14,coast,2 areas on the eastern' of the Adriatic Sea shore,3,coast,2 Thesaurus of Current English' The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2 latedness are annotated without any distinction.\\nSeveral studies indicate that the human scores consistently have very high correlations with each other (Miller and Charles, 1991; Resnik, 1995), thus validating the use of these datasets for evaluating semantic similarity.\\nFor the cross-lingual evaluation, the two datasets were modified by translating the second word in each pair into Spanish.\\nTwo humans translated simultaneously both datasets, with an inter-tagger agreement of 72% for RG and 84% for WordSim353.\\n\\n4.2 Results\\n\\nTable 1 shows the Spearman correlation obtained on the RG and WordSim353 datasets, including the interval at 0.95 of confidence6.\\nOverall the distributional context-window approach performs best in the RG, reaching 0.89 correlation, and both WN30g and the combination of context windows and syntactic context perform best on WordSim353.\\nNote that the confidence intervals are quite large in both RG and WordSim353, and few of the pairwise differences are statistically significant.\\nRegarding WordNet-based approaches, the use of the glosses and WordNet 3.0 (WN30g) yields the best results in both datasets.\\nWhile MCR16 is close to WN30g for the RG dataset, it lags well behind on WordSim353.\\nThis discrepancy is further analyzed is Section 5.3.\\nNote that the performance of WordNet in the WordSim353 dataset suffers from unknown words.\\nIn fact, there are nine pairs which returned null similarity for this reason.\\nThe number in parenthesis in Table 1 for WordSim353 shows the results for the 344 remaining pairs.\\nSection 5.2 shows a proposal to overcome this limitation.\\nThe bag-of-words approach tends to group together terms that can have a similar distribution of contextual terms.\\nTherefore, terms that are topically related can appear in the same textual passages and will get high values using this model.\\nWe see this as an explanation why this model performed better than the context window approach for WordSim353, where annotators were instructed to provide high ratings to related terms.\\nOn the contrary, the context window approach tends to group together words that are exchangeable in exactly the same context, preserving order.\\nTable 2 illustrates a few examples of context collected.\\nTherefore, true synonyms and hyponyms/hyperonyms will receive high similarities, whereas terms related topically or based on any other semantic relation (e.g. movie and star) will have lower scores.\\nThis explains why this method performed better for the RG dataset.\\nSection 5.3 confirms these observations.\\nTable 3 shows the results for the English-Spanish cross-lingual datasets.\\nFor RG, MCR16 and the context windows methods drop only 5 percentage points, showing that cross-lingual similarity is feasible, and that both cross-lingual strategies are robust.\\nThe results for WordSim353 show that WN30g is the best for this dataset, with the rest of the methods falling over 10 percentage points relative to the monolingual experiment.\\nA closer look at the WordNet results showed that most of the drop in performance was caused by out-of-vocabulary words, due to the smaller vocabulary of the Spanish WordNet.\\nThough not totally comparable, if we compute the correlation over pairs covered in WordNet alone, the correlation would drop only 2 percentage points.\\nIn the case of the distributional approaches, the fall in performance was caused by the translations, as only 61% of the words were translated into the original word in the English datasets.\\n\\n5 Detailed analysis and system combination\\n\\nIn this section we present some analysis, including learning curves for distributional methods, the use of distributional similarity to improve WordNet similarity, the contrast between similarity and relatedness, and the combination of methods.\\nFigure 1 shows that the correlation improves with the size of the corpus, as expected.\\nFor the results using the WordSim353 corpus, we show the results of the bag-of-words approach with context size 10.\\nResults improve from 0.5 Spearman correlation up to 0.65 when increasing the corpus size three orders of magnitude, although the effect decays at the end, which indicates that we might not get further gains going beyond the current size of the corpus.\\nWith respect to results for the RG dataset, we used a context-window approach with context radius 4.\\nHere, results improve even more with data size, probably due to the sparse data problem collecting 8-word context windows if the corpus is not large enough.\\nCorrelation improves linearly right to the end, where results stabilize around 0.89.\\nAlthough the vocabulary of WordNet is very extensive, applications are bound to need the similarity between words which are not included in WordNet.\\nThis is exemplified in the WordSim353 dataset, where 9 pairs contain words which are unknown to WordNet.\\nIn order to overcome this shortcoming, we could use similar words instead, as provided by the distributional thesaurus.\\nWe used the distributional thesaurus defined in Section 3, using context windows of width 4, to provide three similar words for each of the unknown words in WordNet.\\nResults improve for both WN30 and WN30g, as shown in Table 4, attaining our best results for WordSim353.\\nWe mentioned above that the annotation guidelines of WordSim353 did not distinguish between similar and related pairs.\\nAs the results in Section 4 show, different techniques are more appropriate to calculate either similarity or relatedness.\\nIn order to study this effect, ideally, we would have two versions of the dataset, where annotators were given precise instructions to distinguish similarity in one case, and relatedness in the other.\\nGiven the lack of such datasets, we devised a simpler approach in order to reuse the existing human judgements.\\nWe manually split the dataset in two parts, as follows.\\nFirst, two humans classified all pairs as being synonyms of each other, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and noneof-the-above.\\nThe inter-tagger agreement rate was 0.80, with a Kappa score of 0.77.\\nThis annotation was used to group the pairs in three categories: similar pairs (those classified as synonyms, antonyms, identical, or hyponym-hyperonym), related pairs (those classified as meronym-holonym, and pairs classified as none-of-the-above, with a human average similarity greater than 5), and unrelated pairs (those classified as none-of-the-above that had average similarity less than or equal to 5).\\nWe then created two new gold-standard datasets: similarity (the union of similar and unrelated pairs), and relatedness (the union of related and unrelated)7.\\nTable 5 shows the results on the relatedness and similarity subsets of WordSim353 for the different methods.\\nRegarding WordNet methods, both WN30 and WN30g perform similarly on the similarity subset, but WN30g obtains the best results by far on the relatedness data.\\nThese results are congruent with our expectations: two words are similar if their synsets are in close places in the WordNet hierarchy, and two words are related if there is a connection between them.\\nMost of the relations in WordNet are of hierarchical nature, and although other relations exist, they are far less numerous, thus explaining the good results for both WN30 and WN30g on similarity, but the bad results of WN30 on relatedness.\\nThe disambiguated glosses help find connections among related concepts, and allow our method to better model relatedness with respect to WN30.\\nThe low results for MCR16 also deserve some comments.\\nGiven the fact that MCR16 performed very well on the RG dataset, it comes as a surprise that it performs so poorly for the similarity subset of WordSim353.\\nIn an additional evaluation, we attested that MCR16 does indeed perform as well as MCR30g on the similar pairs subset.\\nWe believe that this deviation could be due to the method used to construct the similarity dataset, which includes some pairs of loosely related pairs labeled as unrelated.\\nConcerning the techniques based on distributional similarities, the method based on context windows provides the best results for similarity, and the bagof-words representation outperforms most of the other techniques for relatedness.\\nIn order to gain an insight on which would be the upper bound that we could obtain when combining our methods, we took the output of three systems (bag of words with window size 10, context window with size 4, and the WN30g run).\\nEach of these outputs is a ranking of word pairs, and we implemented an oracle that chooses, for each pair, the rank that is most similar to the rank of the pair in the gold-standard.\\nThe outputs of the oracle have a Spearman correlation of 0.97 for RG and 0.92 for WordSim353, which gives as an indication of the correlations that could be achieved by choosing for each pair the rank output by the best classifier for that pair.\\nThe previous results motivated the use of a supervised approach to combine the output of the different systems.\\nWe created a training corpus containing pairs of pairs of words from the datasets, having as features the similarity and rank of each pair involved as given by the different unsupervised systems.\\nA classifier is trained to decide whether the first pair is more similar than the second one.\\nFor example, a training instance using two unsupervised classifiers is 0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative meaning that the similarities given by the first classifier to the two pairs were 0.001364 and 0.327515 respectively, which ranked them in positions 31 and 64.\\nThe second classifier gave them similarities of 0.084805 and 0.109061 respectively, which ranked them in positions 57 and 59.\\nThe class negative indicates that in the gold-standard the first pair has a lower score than the second pair.\\nWe have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets.\\nIt uses a polynomial kernel with degree 4.\\nWe did not have a held-out set, so we used the standard settings of Weka, without trying to modify parameters, e.g.\\nC. Each word pair is scored with the number of pairs that were considered to have less similarity using the SVM.\\nThe results using 10-fold crossvalidation are shown in Table 6.\\nA combination of all methods produces the best results reported so far for both datasets, statistically significant for RG.\\n\\n6 Related work\\n\\nContrary to the WordSim353 dataset, common practice with the RG dataset has been to perform the evaluation with Pearson correlation.\\nIn our believe Pearson is less informative, as the Pearson correlation suffers much when the scores of two systems are not linearly correlated, something which happens often given due to the different nature of the techniques applied.\\nSome authors, e.g.\\nAlvarez and Lim (2007), use a non-linear function to map the system outputs into new values distributed more similarly to the values in the gold-standard.\\nIn their case, the mapping function was exp (−'4 ), which was chosen empirically.\\nFinding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations.\\nAlternatively, the Spearman correlation provides an evaluation metric that is independent of such data-dependent transformations.\\nMost similarity researchers have published their complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation.\\nTable 7 shows the results of related work on MC that was available to us, including our own.\\nFor the authors that did not provide the detailed data we include only the Pearson correlation with no confidence intervals.\\nAmong the unsupervised methods introduced in this paper, the context window produced the best reported Spearman correlation, although the 0.95 confidence intervals are too large to allow us to accept the hypothesis that it is better than all others methods.\\nThe supervised combination produces the best results reported so far.\\nFor the benefit of future research, our results for the MC subset are displayed in Table 8.\\nComparison on the WordSim353 dataset is easier, as all researchers have used Spearman.\\nThe figures in Table 9) show that our WordNet-based method outperforms all previously published WordNet methods.\\nWe want to note that our WordNetbased method outperforms that of Hughes and Ramage (2007), which uses a similar method.\\nAlthough there are some differences in the method, we think that the main performance gain comes from the use of the disambiguated glosses, which they did not use.\\nOur distributional methods also outperform all other corpus-based methods.\\nThe most similar approach to our distributional technique is Finkelstein et al. (2002), who combined distributional similarities from Web documents with a similarity from WordNet.\\nTheir results are probably worse due to the smaller data size (they used 270,000 documents) and the differences in the calculation of the similarities.\\nThe only method which outperforms our non-supervised methods is that of (Gabrilovich and Markovitch, 2007) when based on Wikipedia, probably because of the dense, manually distilled knowledge contained in Wikipedia.\\nAll in all, our supervised combination gets the best published results on this dataset.\\n\\n7 Conclusions and future work\\n\\nThis paper has presented two state-of-the-art distributional and WordNet-based similarity measures, with a study of several parameters, including performance on similarity and relatedness data.\\nWe show that the use of disambiguated glosses allows for the best published results for WordNet-based systems on the WordSim353 dataset, mainly due to the better modeling of relatedness (as opposed to similarity).\\nDistributional similarities have proven to be competitive when compared to knowledgebased methods, with context windows being better for similarity and bag of words for relatedness.\\nDistributional similarity was effectively used to cover out-of-vocabulary items in the WordNet-based measure providing our best unsupervised results.\\nThe complementarity of our methods was exploited by a supervised learner, producing the best results so far for RG and WordSim353.\\nOur results include confidence values, which, surprisingly, were not included in most previous work, and show that many results over RG and WordSim353 are indistinguishable.\\nThe algorithm for WordNet-base similarity and the necessary resources are publicly available8.\\nThis work pioneers cross-lingual extension and evaluation of both distributional and WordNet-based measures.\\nWe have shown that closely aligned wordnets provide a natural and effective way to compute cross-lingual similarity with minor losses.\\nA simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5\",\n",
       " 'summary': 'This paper presents and compares WordNet-based and distributional similarity approaches.\\nThe strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.\\nEach of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.\\nFinally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.\\nWe derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs.\\nExamining the relations between the words in each pair, we further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.',\n",
       " 'title': 'A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that the data is working properly...\n",
    "\n",
    "data_dict['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ef0601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef0cd4",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695f2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d87f16",
   "metadata": {},
   "source": [
    "# Section 2:  Fine Tuning Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c385a82",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, DataCollatorForSeq2Seq \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     41\u001b[0m     add_model_info_to_custom_pipelines,\n\u001b[0;32m     42\u001b[0m     cached_property,\n\u001b[0;32m     43\u001b[0m     can_return_loss,\n\u001b[0;32m     44\u001b[0m     expand_dims,\n\u001b[0;32m     45\u001b[0m     filter_out_non_signature_kwargs,\n\u001b[0;32m     46\u001b[0m     find_labels,\n\u001b[0;32m     47\u001b[0m     flatten_dict,\n\u001b[0;32m     48\u001b[0m     infer_framework,\n\u001b[0;32m     49\u001b[0m     is_jax_tensor,\n\u001b[0;32m     50\u001b[0m     is_numpy_array,\n\u001b[0;32m     51\u001b[0m     is_tensor,\n\u001b[0;32m     52\u001b[0m     is_tf_symbolic_tensor,\n\u001b[0;32m     53\u001b[0m     is_tf_tensor,\n\u001b[0;32m     54\u001b[0m     is_torch_device,\n\u001b[0;32m     55\u001b[0m     is_torch_dtype,\n\u001b[0;32m     56\u001b[0m     is_torch_tensor,\n\u001b[0;32m     57\u001b[0m     reshape,\n\u001b[0;32m     58\u001b[0m     squeeze,\n\u001b[0;32m     59\u001b[0m     strtobool,\n\u001b[0;32m     60\u001b[0m     tensor_size,\n\u001b[0;32m     61\u001b[0m     to_numpy,\n\u001b[0;32m     62\u001b[0m     to_py_obj,\n\u001b[0;32m     63\u001b[0m     torch_float,\n\u001b[0;32m     64\u001b[0m     torch_int,\n\u001b[0;32m     65\u001b[0m     transpose,\n\u001b[0;32m     66\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     70\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     99\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[0;32m    100\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     torch_only_method,\n\u001b[0;32m    220\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:462\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 462\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq \n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "import rouge_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be96cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length = 1024, truncation = True)\n",
    "    labels = tokenizer(text_target = examples[\"summary\"], max_length = 256, truncation = True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbeffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = data_dict.map(preprocess_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model=checkpoint, return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0231a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens = True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    "    result = rouge.compute(predictions = decoded_preds, references = decoded_labels, use_stemmer = True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v, in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65243b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model.generation_config.max_new_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38551b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    num_train_epochs = 3,\n",
    "    \n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    \n",
    "    output_dir = \"T5_model\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    \n",
    "    save_total_limit = 1,\n",
    "    predict_with_generate = True,\n",
    "    fp16 = True,\n",
    "    push_to_hub = False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data[\"train\"],\n",
    "    eval_dataset = tokenized_data[\"valid\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5396e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d2374",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory=\"./T5_model\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "trainer.save_model(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./T5_model\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./T5_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ecd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57952646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(inputs):\n",
    "    inputs = tokenizer(inputs, return_tensors='pt', padding = True, truncation = True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], max_new_tokens = 256, min_new_tokens = 200, num_beams = 4, early_stopping = True)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = []\n",
    "for example in tokenized_data['test']:\n",
    "    input_text = example['text']\n",
    "    prediction = generate_summaries(input_text)\n",
    "    generated_texts.append(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_summaries = [example['summary'] for example in data_dict['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rouge.compute(predictions = generated_texts, references = true_summaries, use_stemmer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7048805",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d79df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_summaries[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d2989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
